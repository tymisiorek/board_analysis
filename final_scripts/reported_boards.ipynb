{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "absolute_path = \"C:\\\\Users\\\\tykun\\\\OneDrive\\\\Documents\\\\SchoolDocs\\\\VSCodeProjects\\\\connectedData\\\\board_analysis\\\\\"\n",
    "altered_dataframes = \"altered_dataframes\\\\\"\n",
    "gpt_dataframes = \"gpt_dataframes\\\\\"\n",
    "graphs = \"graphs\\\\\"       # (Not used for saving in this version)\n",
    "scripts = \"scripts\\\\\"\n",
    "board_dataframes = \"board_dataframes\\\\\"\n",
    "yearly_interlocks = \"yearly_interlocks\\\\\"\n",
    "\n",
    "years = [\"1999\", \"2000\", \"2005\", \"2007\", \"2008\", \"2009\", \"2011\", \"2013\", \"2018\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_samples(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Filter the DataFrame to include only rows where 'PrimarySample' is True.\"\"\"\n",
    "    return df[df['PrimarySample'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 'Polytechnic Institute Of New York University': replacing with canonical Institution 'New York University'\n",
      "Group 'Austin Pay State University': replacing with canonical Institution 'Austin Peay State University'\n",
      "Group 'Loyola University Maryland': replacing with canonical Institution 'Loyola University Maryland'\n",
      "Group 'St Marys University Texas': replacing with canonical Institution 'St Marys University'\n",
      "Group 'State University Of New York College At Plattsburgh': replacing with canonical Institution 'State University Of New York At Plattsburgh'\n",
      "Group 'University Of Illinois System': replacing with canonical Institution 'University Of Illinois System'\n",
      "Group 'Arcadia University': replacing with canonical Institution 'Arcadia University'\n",
      "Group 'Southeast Missouri University': replacing with canonical Institution 'Southeast Missouri State University'\n",
      "Group 'Loyola University New Orleans': replacing with canonical Institution 'Loyola University New Orleans'\n",
      "Group 'Montana State University': replacing with canonical Institution 'Montana State University Bozeman'\n",
      "Group 'University At Albany Suny': replacing with canonical Institution 'University At Albany Suny'\n",
      "Group 'Lenoir Rhyne University': replacing with canonical Institution 'Lenoir Rhyne College'\n",
      "Group 'Minnesota State University Moorhead': replacing with canonical Institution 'Minnesota State University Moorhead'\n",
      "Group 'Union Institute & University': replacing with canonical Institution 'Union Institute & University'\n",
      "Group 'State University Of New York College At Oneonta': replacing with canonical Institution 'State University Of New York College At Oneonta'\n",
      "Group 'University At Buffalo State University Of New York': replacing with canonical Institution 'University At Buffalo State University Of New York'\n",
      "Group 'Notre Dame Of Maryland University': replacing with canonical Institution 'College Of Notre Dame Of Maryland'\n",
      "Group 'Elon University': replacing with canonical Institution 'Elon University'\n",
      "Group 'University Of Mary Washington': replacing with canonical Institution 'University Of Mary Washington'\n",
      "Group 'Mount Saint Marys University': replacing with canonical Institution 'Mount Saint Marys University'\n",
      "Group 'State University Of New York': replacing with canonical Institution 'State University Of New York System'\n",
      "Group 'Southern Illinois University System': replacing with canonical Institution 'Southern Illinois University System'\n",
      "Group 'Marylhurst University': replacing with canonical Institution 'Marylhurst University'\n",
      "Group 'State University Of New York Stony Brook': replacing with canonical Institution 'State University Of New York At Stony Brook'\n",
      "Group 'Longwood University': replacing with canonical Institution 'Longwood University'\n",
      "Group 'Lynchburg College In Virginia': replacing with canonical Institution 'Lynchburg College'\n",
      "Group 'St Catherine University': replacing with canonical Institution 'College Of St Catherine'\n",
      "Group 'City University Of New York Cuny': replacing with canonical Institution 'City University Of New York'\n",
      "Group 'Drury University': replacing with canonical Institution 'Drury University'\n",
      "Group 'Virginia Tech': replacing with canonical Institution 'Virginia Polytechnic Institute And State University'\n",
      "Group 'New School': replacing with canonical Institution 'New School'\n",
      "Group 'Washington & Jefferson College': replacing with canonical Institution 'Washington & Jefferson College'\n",
      "Group 'Queens University Of Charlotte': replacing with canonical Institution 'Queens University Of Charlotte'\n",
      "Group 'Minnesota State University Mankato': replacing with canonical Institution 'Minnesota State University Mankato'\n",
      "Group 'Missouri State University': replacing with canonical Institution 'Missouri State University'\n",
      "Group 'Quinnipiac University': replacing with canonical Institution 'Quinnipiac University'\n",
      "Group 'Whitworth University': replacing with canonical Institution 'Whitworth College'\n",
      "\n",
      "Merged boards saved to: merged_boards\\merged_boards.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def remove_non_samples(df):\n",
    "    # Your cleaning logic here…\n",
    "    return df\n",
    "\n",
    "def normalize(x):\n",
    "    \"\"\"Convert the input to a string and strip whitespace.\"\"\"\n",
    "    return str(x).strip()\n",
    "\n",
    "# ======================\n",
    "# --- UNION–FIND SETUP ---\n",
    "# ======================\n",
    "# We'll use union–find to group identifiers (Institution names and AffiliationIds)\n",
    "parent = {}\n",
    "appearances = {}   # Map: normalized node -> set of years it appears in.\n",
    "node_sources = {}  # Map: normalized node -> set of source tags (\"Institution\", \"AffiliationId\")\n",
    "\n",
    "def find(x):\n",
    "    x = normalize(x)\n",
    "    if parent[x] != x:\n",
    "        parent[x] = find(parent[x])\n",
    "    return parent[x]\n",
    "\n",
    "def union(x, y):\n",
    "    x = normalize(x)\n",
    "    y = normalize(y)\n",
    "    rootX = find(x)\n",
    "    rootY = find(y)\n",
    "    if rootX != rootY:\n",
    "        parent[rootY] = rootX\n",
    "\n",
    "def add_node(node, source, year):\n",
    "    node = normalize(node)\n",
    "    if node == \"\":\n",
    "        return\n",
    "    if node not in parent:\n",
    "        parent[node] = node\n",
    "    appearances.setdefault(node, set()).add(year)\n",
    "    node_sources.setdefault(node, set()).add(source)\n",
    "\n",
    "# ----------------------------\n",
    "# Record associations between AffiliationId and Institution.\n",
    "# For rows where both values are present, we record that this affiliation id\n",
    "# is paired with the institution name.\n",
    "aff_to_inst = defaultdict(set)\n",
    "\n",
    "# ======================\n",
    "# --- MERGE THE DATA ---\n",
    "# ======================\n",
    "# We'll store all the boards in one DataFrame.\n",
    "merged_boards_list = []  # to hold DataFrames for each year\n",
    "\n",
    "for year in years:\n",
    "    file_path = f\"{absolute_path}{board_dataframes}{year}_boards.csv\"\n",
    "    boards_df = pd.read_csv(file_path)\n",
    "    boards_df = remove_non_samples(boards_df)\n",
    "    boards_df['Year'] = year  # add a Year column\n",
    "    merged_boards_list.append(boards_df)\n",
    "    \n",
    "    # Process each row for union–find\n",
    "    for _, row in boards_df.iterrows():\n",
    "        inst = row.get(\"Institution\")\n",
    "        aff = row.get(\"AffiliationId\")\n",
    "        \n",
    "        valid_inst = pd.notna(inst) and normalize(inst) != \"\"\n",
    "        valid_aff  = pd.notna(aff)  and normalize(aff)  != \"\"\n",
    "        \n",
    "        # If Institution is present, record it.\n",
    "        if valid_inst:\n",
    "            add_node(inst, \"Institution\", year)\n",
    "        \n",
    "        # For AffiliationId:\n",
    "        # - If only AffiliationId is present, record it.\n",
    "        # - If both are present, record both and union them.\n",
    "        if valid_aff:\n",
    "            if not valid_inst:\n",
    "                add_node(aff, \"AffiliationId\", year)\n",
    "            else:\n",
    "                add_node(aff, \"AffiliationId\", year)\n",
    "                aff_to_inst[normalize(aff)].add(normalize(inst))\n",
    "                union(inst, aff)\n",
    "\n",
    "# Concatenate all boards into a single DataFrame.\n",
    "merged_df = pd.concat(merged_boards_list, ignore_index=True)\n",
    "\n",
    "# ======================\n",
    "# --- GROUP INTO COMPONENTS ---\n",
    "# ======================\n",
    "components = {}\n",
    "for node in parent:\n",
    "    root = find(node)\n",
    "    if root not in components:\n",
    "        components[root] = {\"nodes\": set(), \"years\": set(), \"sources\": set()}\n",
    "    components[root][\"nodes\"].add(node)\n",
    "    if node in appearances:\n",
    "        components[root][\"years\"].update(appearances[node])\n",
    "    if node in node_sources:\n",
    "        components[root][\"sources\"].update(node_sources[node])\n",
    "\n",
    "# ======================\n",
    "# --- IDENTIFY DISCREPANCY GROUPS ---\n",
    "# ======================\n",
    "# A group is flagged if:\n",
    "#   1) It has more than one distinct Institution name (nodes from \"Institution\"), or\n",
    "#   2) The group appears in some year(s) with only an AffiliationId (i.e. no direct Institution).\n",
    "discrepancy_groups = set()\n",
    "for comp_id, data in components.items():\n",
    "    # Get all nodes that came from the \"Institution\" column.\n",
    "    inst_nodes = {node for node in data['nodes'] if \"Institution\" in node_sources.get(node, set())}\n",
    "    # Skip groups that never had any direct Institution data.\n",
    "    if not inst_nodes:\n",
    "        continue\n",
    "    # Get the years where a direct Institution was reported.\n",
    "    direct_inst_years = set()\n",
    "    for node in inst_nodes:\n",
    "        direct_inst_years.update(appearances.get(node, set()))\n",
    "    # If more than one Institution name appears or there are years with only an AffiliationId, flag it.\n",
    "    if (len(inst_nodes) > 1) or (data['years'] != direct_inst_years):\n",
    "        discrepancy_groups.add(comp_id)\n",
    "\n",
    "# ======================\n",
    "# --- ASSIGN GROUP IDs TO THE MERGED DATAFRAME ---\n",
    "# ======================\n",
    "def get_group_id(row):\n",
    "    # Use the nonempty field to compute the group.\n",
    "    if pd.notna(row.get(\"Institution\")) and normalize(row.get(\"Institution\")) != \"\":\n",
    "        return find(normalize(row.get(\"Institution\")))\n",
    "    elif pd.notna(row.get(\"AffiliationId\")) and normalize(row.get(\"AffiliationId\")) != \"\":\n",
    "        return find(normalize(row.get(\"AffiliationId\")))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "merged_df[\"GroupID\"] = merged_df.apply(get_group_id, axis=1)\n",
    "\n",
    "# ======================\n",
    "# --- DETERMINE THE CANONICAL INSTITUTION NAME FOR EACH DISCREPANT GROUP ---\n",
    "# ======================\n",
    "# For each discrepancy group, choose the institution name that appears most frequently\n",
    "# (i.e. the mode among nonempty Institution entries in that group).\n",
    "group_to_canonical = {}\n",
    "for group_id in discrepancy_groups:\n",
    "    group_rows = merged_df[merged_df[\"GroupID\"] == group_id]\n",
    "    # Only consider rows with a nonempty Institution value.\n",
    "    valid_insts = group_rows[\"Institution\"].dropna().apply(normalize)\n",
    "    valid_insts = valid_insts[valid_insts != \"\"]\n",
    "    if not valid_insts.empty:\n",
    "        canonical_name = valid_insts.value_counts().idxmax()\n",
    "        group_to_canonical[group_id] = canonical_name\n",
    "        print(f\"Group '{group_id}': replacing with canonical Institution '{canonical_name}'\")\n",
    "    else:\n",
    "        # In the unlikely case no row has an Institution value, leave it as is.\n",
    "        group_to_canonical[group_id] = None\n",
    "\n",
    "# ======================\n",
    "# --- UPDATE THE MERGED DATAFRAME ---\n",
    "# ======================\n",
    "# For every row that belongs to a discrepant group, replace its Institution column\n",
    "# with the canonical name computed.\n",
    "def update_institution(row):\n",
    "    group_id = row[\"GroupID\"]\n",
    "    if pd.notna(group_id) and group_id in group_to_canonical and group_to_canonical[group_id]:\n",
    "        return group_to_canonical[group_id]\n",
    "    else:\n",
    "        return row.get(\"Institution\")\n",
    "\n",
    "merged_df[\"Institution\"] = merged_df.apply(update_institution, axis=1)\n",
    "\n",
    "# Optionally drop the GroupID column if you no longer need it.\n",
    "merged_df = merged_df.drop(columns=[\"GroupID\"])\n",
    "merged_df = merged_df.drop(columns=[\"FullName\"])\n",
    "\n",
    "# ======================\n",
    "# --- WRITE THE MERGED DATAFRAME TO CSV ---\n",
    "# ======================\n",
    "# Ensure the output folder exists.\n",
    "output_folder = \"merged_boards\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "output_path = os.path.join(output_folder, \"merged_boards.csv\")\n",
    "merged_df.to_csv(output_path, index=False)\n",
    "print(f\"\\nMerged boards saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing year: 1999\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 65\u001b[0m\n\u001b[0;32m     62\u001b[0m     board_member_dict[name]\u001b[38;5;241m.\u001b[39madd(institution)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m boards_df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m---> 65\u001b[0m     \u001b[43mprocess_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m double_boards_df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m     67\u001b[0m     process_row(row)\n",
      "Cell \u001b[1;32mIn[67], line 60\u001b[0m, in \u001b[0;36mprocess_row\u001b[1;34m(row)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvacant\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mName\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mlower():\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[43mclean_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mName\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m institution \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInstitution\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     62\u001b[0m board_member_dict[name]\u001b[38;5;241m.\u001b[39madd(institution)\n",
      "Cell \u001b[1;32mIn[67], line 20\u001b[0m, in \u001b[0;36mclean_name\u001b[1;34m(raw_name)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m title \u001b[38;5;129;01min\u001b[39;00m substrings_to_remove:\n\u001b[0;32m     19\u001b[0m     title_clean \u001b[38;5;241m=\u001b[39m title\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m---> 20\u001b[0m     raw_name \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mescape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtitle_clean\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIGNORECASE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m raw_name \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, raw_name)  \u001b[38;5;66;03m# remove punctuation\u001b[39;00m\n\u001b[0;32m     22\u001b[0m cleaned_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(raw_name\u001b[38;5;241m.\u001b[39msplit())      \u001b[38;5;66;03m# remove extra whitespace\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\re\\__init__.py:185\u001b[0m, in \u001b[0;36msub\u001b[1;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msub\u001b[39m(pattern, repl, string, count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    179\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the string obtained by replacing the leftmost\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;124;03m    non-overlapping occurrences of the pattern in string by the\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;124;03m    replacement repl.  repl can be either a string or a callable;\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m    if a string, backslash escapes in it are processed.  If it is\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m    a callable, it's passed the Match object and must return\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;124;03m    a replacement string to be used.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msub(repl, string, count)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\re\\__init__.py:272\u001b[0m, in \u001b[0;36m_compile\u001b[1;34m(pattern, flags)\u001b[0m\n\u001b[0;32m    269\u001b[0m _cache \u001b[38;5;241m=\u001b[39m {}  \u001b[38;5;66;03m# ordered!\u001b[39;00m\n\u001b[0;32m    271\u001b[0m _MAXCACHE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m\n\u001b[1;32m--> 272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_compile\u001b[39m(pattern, flags):\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;66;03m# internal: compile pattern\u001b[39;00m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(flags, RegexFlag):\n\u001b[0;32m    275\u001b[0m         flags \u001b[38;5;241m=\u001b[39m flags\u001b[38;5;241m.\u001b[39mvalue\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "Get list of interlocked names, and fuzzy match them with other board members to find potential missed interlocking. \n",
    "'''\n",
    "\n",
    "substrings_to_remove = [\n",
    "    \"Rev.\", \"SJ\", \"Sister\", \"Brother\", \"Father\", \"OP\", \"The Very\",\n",
    "    \"Sr.\", \"O.P.\", \"Very Rev.\", \"Br.\", \"Dr.\", \"Md.\", \"S.J.\", \"Very Rev\",\n",
    "    \"M.D.\", \"O.P\", \"S.J\", \"J.R\", \"Jr.\", \"Jr \", \"III\", \"His \", \"Eminence\", \"Cardinal \"\n",
    "]\n",
    "\n",
    "def clean_name(raw_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean and canonicalize a board member's name by:\n",
    "      - Removing specified title substrings (case-insensitive)\n",
    "      - Removing punctuation and extra whitespace\n",
    "      - Converting to title case.\n",
    "    \"\"\"\n",
    "    for title in substrings_to_remove:\n",
    "        title_clean = title.strip()\n",
    "        raw_name = re.sub(r'\\b' + re.escape(title_clean) + r'\\b', '', raw_name, flags=re.IGNORECASE)\n",
    "    raw_name = re.sub(r'[^\\w\\s]', '', raw_name)  # remove punctuation\n",
    "    cleaned_name = \" \".join(raw_name.split())      # remove extra whitespace\n",
    "    return cleaned_name.title()\n",
    "\n",
    "# ---------------------------\n",
    "# Main Processing: Counting Interlocks and Recording Board Members\n",
    "# ---------------------------\n",
    "\n",
    "# We'll store information on interlocked board members from every year.\n",
    "# Each record will contain the Year, a canonical name, the fuzzy–matched names, \n",
    "# the union of institutions (i.e. the interlock set), and a count.\n",
    "all_interlock_members = []\n",
    "\n",
    "for year in years:\n",
    "    print(f\"Processing year: {year}\")\n",
    "    \n",
    "    # Reinitialize the board membership dictionary for this year.\n",
    "    # Key: cleaned board member name; Value: set of institutions where that person appears.\n",
    "    board_member_dict = defaultdict(set)\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Load Board Data (assumes two files per year)\n",
    "    # ---------------------------\n",
    "    boards_path = f\"{absolute_path}{board_dataframes}{year}_boards.csv\"\n",
    "    double_boards_path = f\"{absolute_path}{board_dataframes}{year}_double_board.csv\"\n",
    "    \n",
    "    boards_df = pd.read_csv(boards_path)\n",
    "    double_boards_df = pd.read_csv(double_boards_path)\n",
    "    \n",
    "    boards_df = remove_non_samples(boards_df)\n",
    "    double_boards_df = remove_non_samples(double_boards_df)\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Process rows to build board_member_dict.\n",
    "    # (We ignore any row whose 'Name' contains \"vacant\".)\n",
    "    # ---------------------------\n",
    "    def process_row(row):\n",
    "        if \"vacant\" in row['Name'].lower():\n",
    "            return\n",
    "        name = clean_name(row['Name'])\n",
    "        institution = row['Institution']\n",
    "        board_member_dict[name].add(institution)\n",
    "    \n",
    "    for _, row in boards_df.iterrows():\n",
    "        process_row(row)\n",
    "    for _, row in double_boards_df.iterrows():\n",
    "        process_row(row)\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Identify board members who serve on more than one institution.\n",
    "    # These are our interlock candidates.\n",
    "    # ---------------------------\n",
    "    interlock_candidates = {name: insts for name, insts in board_member_dict.items() if len(insts) > 1}\n",
    "    print(f\"  Found {len(interlock_candidates)} board members with interlocks (before fuzzy matching).\")\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Fuzzy Matching to Merge Names That Might Represent the Same Person\n",
    "    # ---------------------------\n",
    "    # For example, if \"John A. Smith\" and \"John Smith\" both appear, we may want to treat them as the same person.\n",
    "    fuzzy_threshold = 90  # Adjust as needed (0-100)\n",
    "    names_list = list(interlock_candidates.keys())\n",
    "    merged_members = {}  # Mapping: canonical name -> dict with keys: 'Fuzzy_Matches' and 'Institutions'\n",
    "    visited = set()\n",
    "    \n",
    "    for i, name in enumerate(names_list):\n",
    "        if name in visited:\n",
    "            continue\n",
    "        canonical = name\n",
    "        fuzzy_matches = {name}  # start with itself\n",
    "        union_insts = set(interlock_candidates[name])\n",
    "        # Compare with later names.\n",
    "        for other in names_list[i+1:]:\n",
    "            if other in visited:\n",
    "                continue\n",
    "            score = fuzz.token_set_ratio(name, other)\n",
    "            if score >= fuzzy_threshold:\n",
    "                fuzzy_matches.add(other)\n",
    "                union_insts.update(interlock_candidates[other])\n",
    "                visited.add(other)\n",
    "        merged_members[canonical] = {\n",
    "            'Fuzzy_Matches': list(fuzzy_matches),\n",
    "            'Institutions': list(union_insts),\n",
    "            'Interlock_Count': len(union_insts)\n",
    "        }\n",
    "        visited.add(name)\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Record each merged board member as an interlock record.\n",
    "    # ---------------------------\n",
    "    for canonical, info in merged_members.items():\n",
    "        member_record = {\n",
    "            'Year': year,\n",
    "            'Canonical_Name': canonical,\n",
    "            'Fuzzy_Matches': info['Fuzzy_Matches'],\n",
    "            'Institutions': info['Institutions'],\n",
    "            'Interlock_Count': info['Interlock_Count']\n",
    "        }\n",
    "        all_interlock_members.append(member_record)\n",
    "\n",
    "# ---------------------------\n",
    "# Create a DataFrame of All Interlock Board Members\n",
    "# ---------------------------\n",
    "interlock_members_df = pd.DataFrame(all_interlock_members)\n",
    "print(\"\\nInterlock Members DataFrame (first few rows):\")\n",
    "print(interlock_members_df.head())\n",
    "\n",
    "# ---------------------------\n",
    "# Write the DataFrame to CSV\n",
    "# ---------------------------\n",
    "output_folder = \"interlock_results\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "output_path = os.path.join(output_folder, \"interlock_members.csv\")\n",
    "interlock_members_df.to_csv(output_path, index=False)\n",
    "print(f\"\\nInterlock members saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# determine what universities are reported for each year\n",
    "# '''\n",
    "# institutions_by_year = {}\n",
    "# for year in years:\n",
    "#     boards_df = pd.read_csv(f\"{absolute_path}{board_dataframes}{year}_boards.csv\")\n",
    "#     boards_df = remove_non_samples(boards_df)\n",
    "\n",
    "#     institutions_by_year[year] = set(boards_df['Institution'].unique())\n",
    "# common_institutions = set.intersection(*institutions_by_year.values())\n",
    "\n",
    "# # Build a mapping of each institution to the list of years it appears in.\n",
    "# institution_years = {}\n",
    "# for year, inst_set in institutions_by_year.items():\n",
    "#     for institution in inst_set:\n",
    "#         institution_years.setdefault(institution, []).append(year)\n",
    "\n",
    "# # Identify institutions that do not appear in every year.\n",
    "# # (i.e. their years list is not equal to the full list of years)\n",
    "# non_common_institutions = {\n",
    "#     institution: yrs \n",
    "#     for institution, yrs in institution_years.items() \n",
    "#     if set(yrs) != set(years)\n",
    "# }\n",
    "\n",
    "# # Print the results.\n",
    "# print(\"Institutions that appear in every year:\")\n",
    "# print(len(common_institutions),\", \", common_institutions)\n",
    "\n",
    "# print(\"\\nInstitutions that do not appear in every year (with the years they appear):\")\n",
    "# for institution, yrs in non_common_institutions.items():\n",
    "#     print(f\"{institution}: {yrs}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projectEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
